{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Product Browse Node Classification - Amazon ML Hackathon.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "whz6wTm8UUrb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiEu8ER44cA1"
      },
      "source": [
        "#!pip install pandarallel\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqFS2sBqyVas",
        "outputId": "86f109c2-9df8-4716-ddee-fb819d7d56cd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD8HuHzyx3RD"
      },
      "source": [
        "## Prediction set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtQn35z2y3w4"
      },
      "source": [
        "# test_data = pd.read_csv(\"/content/drive/MyDrive/Amazon ML Hackathon/dataset/test.csv\", escapechar = \"\\\\\", quoting = 3)\n",
        "# pd.DataFrame(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPi-xfFO3ocv"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaQw6sfqyWmg"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/Amazon ML Hackathon/dataset/train.csv\", escapechar = \"\\\\\", quoting = 3)\n",
        "data = pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "nHJ365WN4nyP",
        "outputId": "bd755fda-1ced-43b9-ff43-ae615a5783e0"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TITLE</th>\n",
              "      <th>DESCRIPTION</th>\n",
              "      <th>BULLET_POINTS</th>\n",
              "      <th>BRAND</th>\n",
              "      <th>BROWSE_NODE_ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pete The Cat Bedtime Blues Doll, 14.5 Inch</td>\n",
              "      <td>Pete the Cat is the coolest, most popular cat ...</td>\n",
              "      <td>[Pete the Cat Bedtime Blues plush doll,Based o...</td>\n",
              "      <td>MerryMakers</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The New Yorker NYHM014 Refrigerator Magnet, 2 ...</td>\n",
              "      <td>The New Yorker Handsome Cello Wrapped Hard Mag...</td>\n",
              "      <td>[Cat In A Tea Cup by New Yorker cover artist G...</td>\n",
              "      <td>The New Yorker</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Ultimate Self-Sufficiency Handbook: A Comp...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Skyhorse Publishing</td>\n",
              "      <td>imusti</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Amway Nutrilite Kids Chewable Iron Tablets (100)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[Nutrilite Kids,Chewable Iron Tablets,Quantity...</td>\n",
              "      <td>Amway</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Teacher Planner Company A4 6 Lesson Academic T...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903019</th>\n",
              "      <td>Premium Aviator Sunglasses - HD Polarized (Bri...</td>\n",
              "      <td>These premium Aviator Sunglasses with 5 color ...</td>\n",
              "      <td>[Frame size: Lens height - 56mm, Lens width - ...</td>\n",
              "      <td>Generic</td>\n",
              "      <td>1040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903020</th>\n",
              "      <td>Social Distance Stickers - Set of 5 Sticker Sl...</td>\n",
              "      <td>set of 5 prints social distancing sticker self...</td>\n",
              "      <td>[covid19 safety sticker - set of 5 to maintain...</td>\n",
              "      <td>Generic</td>\n",
              "      <td>15199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903021</th>\n",
              "      <td>Torr-to Face Shield PACK OF 5 with Adjustable ...</td>\n",
              "      <td>* COMPLETE FACE PROTECTION: Torr-to Face Shiel...</td>\n",
              "      <td>[350 MICRONS PACK OF 5 PCS,COMPLETE FACE PROTE...</td>\n",
              "      <td>TORR-TO</td>\n",
              "      <td>1044933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903022</th>\n",
              "      <td>Type-C to 3.5 MM for Oppo R17 Pro Type-C to 3....</td>\n",
              "      <td>Still want to use your favorite earphones/head...</td>\n",
              "      <td>[Indian Connectors: Made for Indian sockets, t...</td>\n",
              "      <td>SHOPBELL</td>\n",
              "      <td>14790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2903023</th>\n",
              "      <td>KNG Orange Polo Tshirt (38)</td>\n",
              "      <td>Performance inspired by comfort. The polo shir...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>KNG</td>\n",
              "      <td>1213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2903024 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     TITLE  ... BROWSE_NODE_ID\n",
              "0               Pete The Cat Bedtime Blues Doll, 14.5 Inch  ...              0\n",
              "1        The New Yorker NYHM014 Refrigerator Magnet, 2 ...  ...              1\n",
              "2        The Ultimate Self-Sufficiency Handbook: A Comp...  ...              2\n",
              "3         Amway Nutrilite Kids Chewable Iron Tablets (100)  ...              3\n",
              "4        Teacher Planner Company A4 6 Lesson Academic T...  ...              4\n",
              "...                                                    ...  ...            ...\n",
              "2903019  Premium Aviator Sunglasses - HD Polarized (Bri...  ...           1040\n",
              "2903020  Social Distance Stickers - Set of 5 Sticker Sl...  ...          15199\n",
              "2903021  Torr-to Face Shield PACK OF 5 with Adjustable ...  ...        1044933\n",
              "2903022  Type-C to 3.5 MM for Oppo R17 Pro Type-C to 3....  ...          14790\n",
              "2903023                        KNG Orange Polo Tshirt (38)  ...           1213\n",
              "\n",
              "[2903024 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgVxMBYwM0TG"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmVUrMU57BYt"
      },
      "source": [
        "#Dropping the description column\n",
        "data = data.drop(\"DESCRIPTION\", axis = 1)\n",
        "\n",
        "#Removing duplicate titles but ensuring we dont drop any unique browse node id\n",
        "data = data.drop_duplicates(subset=[\"TITLE\",\"BROWSE_NODE_ID\"])\n",
        "\n",
        "#Dropping the null titles\n",
        "data = data.dropna(subset=[\"TITLE\"])\n",
        "\n",
        "data.BULLET_POINTS = data.BULLET_POINTS.fillna(\" \")\n",
        "# data[\"TBP\"] = data[\"TITLE\"].astype(str) + \" \" +data[\"BULLET_POINTS\"].astype(str)\n",
        "\n",
        "# data = data.drop(columns=[\"BULLET_POINTS\"])\n",
        "data.BRAND = data.BRAND.fillna(\" \")\n",
        "data[\"TBP\"] = data[\"BRAND\"].astype(str) + \" \" +data[\"BULLET_POINTS\"].astype(str)\n",
        "\n",
        "data = data.drop(columns=[\"BULLET_POINTS\", \"BRAND\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzpbHTIKYe0C",
        "outputId": "77f3fc70-a9be-40e5-e224-a635279fa9bf"
      },
      "source": [
        "data, data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                     TITLE  ...                                                TBP\n",
              " 0               Pete The Cat Bedtime Blues Doll, 14.5 Inch  ...  MerryMakers [Pete the Cat Bedtime Blues plush ...\n",
              " 1        The New Yorker NYHM014 Refrigerator Magnet, 2 ...  ...  The New Yorker [Cat In A Tea Cup by New Yorker...\n",
              " 2        The Ultimate Self-Sufficiency Handbook: A Comp...  ...                         imusti Skyhorse Publishing\n",
              " 3         Amway Nutrilite Kids Chewable Iron Tablets (100)  ...  Amway [Nutrilite Kids,Chewable Iron Tablets,Qu...\n",
              " 4        Teacher Planner Company A4 6 Lesson Academic T...  ...                                                   \n",
              " ...                                                    ...  ...                                                ...\n",
              " 2903019  Premium Aviator Sunglasses - HD Polarized (Bri...  ...  Generic [Frame size: Lens height - 56mm, Lens ...\n",
              " 2903020  Social Distance Stickers - Set of 5 Sticker Sl...  ...  Generic [covid19 safety sticker - set of 5 to ...\n",
              " 2903021  Torr-to Face Shield PACK OF 5 with Adjustable ...  ...  TORR-TO [350 MICRONS PACK OF 5 PCS,COMPLETE FA...\n",
              " 2903022  Type-C to 3.5 MM for Oppo R17 Pro Type-C to 3....  ...  SHOPBELL [Indian Connectors: Made for Indian s...\n",
              " 2903023                        KNG Orange Polo Tshirt (38)  ...                                              KNG  \n",
              " \n",
              " [2756727 rows x 3 columns], (2756727, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjB0DxUix3CH"
      },
      "source": [
        "def string_cleaning(string):\n",
        "    string = list(string.lower())\n",
        "    for i in string:\n",
        "      if i == \",\":\n",
        "        string[string.index(i)] = \" \"\n",
        "      if i in \"[]:;\":\n",
        "        string.remove(i)\n",
        "    string = \"\".join(string)\n",
        "    output = []\n",
        "    string = string.split(\" \")\n",
        "    for i in string:\n",
        "      if i not in output:\n",
        "        output.append(i)\n",
        "    output = \" \".join(output)\n",
        "    \n",
        "    return output\n",
        "\n",
        "data.TBP = data.TBP.apply(string_cleaning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NzU50L-oLse",
        "outputId": "57027607-1fbe-4b41-84fa-f43ef0eb3c5b"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "set(stopwords.words('english'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMWfXbyooEOV"
      },
      "source": [
        "def remove_stopwords(string):\n",
        "    stop_words = set(stopwords.words('english'))  \n",
        "    word_tokens = word_tokenize(string)    \n",
        "        \n",
        "    filtered_sentence = [] \n",
        "      \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w) \n",
        "\n",
        "    return \" \".join(filtered_sentence)\n",
        "\n",
        "data.TBP = data.TBP.apply(remove_stopwords)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjVkkHShS_wS"
      },
      "source": [
        "data.to_csv(\"/content/drive/MyDrive/Amazon ML Hackathon/checkpoint_nltk.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whz6wTm8UUrb"
      },
      "source": [
        "## word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvKi_nox3iY7"
      },
      "source": [
        "# import gensim.downloader as api\n",
        "# wv=api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9fw1bQqlUYY"
      },
      "source": [
        "# TBP=data.TBP.tolist()\n",
        "# #TITLE=data.TITLE.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7kLejjEmVpW"
      },
      "source": [
        "# from gensim.models.callbacks import CallbackAny2Vec\n",
        "# from gensim.models import Word2Vec\n",
        "\n",
        "# # init callback class\n",
        "# class callback(CallbackAny2Vec):\n",
        "#     \"\"\"\n",
        "#     Callback to print loss after each epoch\n",
        "#     \"\"\"\n",
        "#     def __init__(self):\n",
        "#         self.epoch = 0\n",
        "\n",
        "#     def on_epoch_end(self, model):\n",
        "#         loss = model.get_latest_training_loss()\n",
        "        \n",
        "#         if self.epoch == 0:\n",
        "#             print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "#         elif self.epoch % 1 == 0:\n",
        "#             print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "        \n",
        "        \n",
        "#         self.epoch += 1\n",
        "#         self.loss_previous_step = loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS7z-wkNdRJX"
      },
      "source": [
        "# def train_data(sentences):\n",
        "#     w2v_model = Word2Vec(size = 300,\n",
        "#                         window = 20,\n",
        "#                         min_count = 2,\n",
        "#                         workers = 20,\n",
        "#                         sg = 0,\n",
        "#                         negative = 5,\n",
        "#                         sample = 1e-5)\n",
        "\n",
        "#     w2v_model.build_vocab(sentences)\n",
        "\n",
        "      \n",
        "#     # train the w2v model\n",
        "#     w2v_model.train(sentences, \n",
        "#                     total_examples=w2v_model.corpus_count, \n",
        "#                     epochs=100, \n",
        "#                     report_delay=1,\n",
        "#                     compute_loss = True, # set compute_loss = True\n",
        "#                     callbacks=[callback()]) # add the callback class\n",
        "                  \n",
        "#     return w2v_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPYUznSTmKcl"
      },
      "source": [
        "# model_TBP=train_data(TBP)\n",
        "# #model_title=train_data(TITLE)\n",
        "# model_TBP.save(\"/content/drive/MyDrive/Amazon ML Hackathon/w2vtbp.model\")\n",
        "# #model_title.save(\"/content/drive/MyDrive/Amazon ML Hackathon/w2vtitle.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AuSOYXoUJ-_"
      },
      "source": [
        "# #encoded_docs = [[model_TBP.wv[word] for word in post] for post in TBP]\n",
        "# model_TBP = Word2Vec.load(\"/content/drive/MyDrive/Amazon ML Hackathon/w2vtbp.model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfilRwjRUGlX",
        "outputId": "62c23bb4-cf3b-4506-bb3f-36162c2eede5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Amazon ML Hackathon/checkpoint_nltk.csv\")\n",
        "data.TBP = data.TBP.fillna(\"0\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I4joDegUY0-"
      },
      "source": [
        "## hashin vectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYtxPUh0Ux9w"
      },
      "source": [
        "hash_length = 100\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "titles = data.TITLE.tolist()\n",
        "tbp = data.TBP.tolist()\n",
        "\n",
        "vectorizer = HashingVectorizer(norm = None,n_features = hash_length)\n",
        "vectorizer.fit(titles, tbp)\n",
        "titles = vectorizer.transform(titles)\n",
        "tbp = vectorizer.transform(tbp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "pvZadtU-kSCE",
        "outputId": "da48d7c4-442c-4933-ba2d-2141711f452c"
      },
      "source": [
        "tbp = np.array(tbp, dtype = np.float16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'csr_matrix'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-9104cfc55761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtbp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaXIcBs2Vhq5"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "nClasses = 9919\n",
        "input = Input(shape =(300,2))\n",
        "\n",
        "#YOLO CNN Architecture\n",
        "conv = Conv1D(64,7, strides = 2, activation = 'relu')(input)\n",
        "maxp = MaxPool1D(strides = 2)(conv)\n",
        "\n",
        "conv = Conv1D(192, 3, strides = 2, activation= 'relu')(maxp)\n",
        "maxp = MaxPool1D(strides = 2)(conv)\n",
        "\n",
        "conv = Conv1D(128,1, activation= 'relu')(maxp)\n",
        "conv = Conv1D(256, 3, activation= 'relu')(conv)\n",
        "conv = Conv1D(256, 1, activation= 'relu')(conv)\n",
        "conv = Conv1D(512, 3, activation= 'relu')(conv)\n",
        "maxp = MaxPool1D(strides = 2)(conv)\n",
        "\n",
        "conv = Conv1D(256,1 ,activation= 'relu')(maxp)\n",
        "conv = Conv1D(512, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(256,1 ,activation= 'relu')(conv)\n",
        "conv = Conv1D(512, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(256,1 ,activation= 'relu')(conv)\n",
        "conv = Conv1D(512, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(256,1 ,activation= 'relu')(conv)\n",
        "conv = Conv1D(512, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(512, 1, activation = 'relu')(conv)\n",
        "conv = Conv1D(1024, 3, activation = 'relu')(conv)\n",
        "maxp = MaxPool1D(strides = 2)(conv)\n",
        "\n",
        "conv = Conv1D(512,1 ,activation= 'relu')(maxp)\n",
        "conv = Conv1D(1024, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(512,1 ,activation= 'relu')(conv)\n",
        "conv = Conv1D(1024, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(1024, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(1024, 3, strides = 2, activation = 'relu')(conv)\n",
        "\n",
        "conv = Conv1D(1024, 3, activation = 'relu')(conv)\n",
        "conv = Conv1D(1024, 3, activation = 'relu')(conv)\n",
        "\n",
        "output = Dense(nClasses, activation = 'softmax')(conv)\n",
        "\n",
        "model = keras.Model(inputs = input, outputs = output)\n",
        "model.summary()\n",
        "\n",
        "opt= keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}